{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7fc2a0f3ba20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load from csv file images and labels, split them into a train/validation data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26061\n"
     ]
    }
   ],
   "source": [
    "img_table = pd.read_csv('/data/VPS/VPS_03/lmq/pancreas/pancr_net.csv')\n",
    "X = np.array(img_table['Path'])\n",
    "Y = np.array(img_table['var_0'])\n",
    "x_train, X_test, y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2022)\n",
    "print(len(X))\n",
    "train_df = pd.DataFrame({'Path':x_train, 'var':y_train})\n",
    "val_df = pd.DataFrame({'Path':X_test, 'var':Y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "\n",
    "IMAGENET_SIZE = 224\n",
    "\n",
    "train_transform = albumentations.Compose([\n",
    "        \n",
    "    albumentations.Resize(IMAGENET_SIZE, IMAGENET_SIZE),\n",
    "    # albumentations.RandomCrop(512, 512),\n",
    "        # illumilation\n",
    "    # albumentations.JpegCompression(quality_lower=99, quality_upper=100,p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomGamma(gamma_limit=(60, 120), p=0.9),\n",
    "        albumentations.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.9),\n",
    "        albumentations.RandomBrightness(limit=0.2, p=0.9),\n",
    "        albumentations.RandomContrast(limit=0.2, p=0.9)\n",
    "        ]),\n",
    "#                                    CLAHE(clip_limit=4.0, tile_grid_size=(3, 3), p=1)\n",
    "    # albumentations.GaussNoise(var_limit=(10, 30), p=0.5),\n",
    "    \n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, interpolation=cv2.INTER_LINEAR,border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "#                                        OpticalDistortion(distort_limit=0.05, shift_limit=0.05, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101, p=1)\n",
    "    albumentations.OneOf([\n",
    "        albumentations.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "        albumentations.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=1)\n",
    "    ], p=0.5),\n",
    "    albumentations.Normalize(mean=(0.485, 0.12, 0.005), std=(0.081, 0.32, 0.043), max_pixel_value=255.0, p=1.0),\n",
    "])\n",
    "\n",
    "val_transform = albumentations.Compose([\n",
    "    albumentations.Resize(IMAGENET_SIZE, IMAGENET_SIZE),\n",
    "    # albumentations.CenterCrop(512, 512),\n",
    "    # albumentations.JpegCompression(quality_lower=99, quality_upper=100,p=1),\n",
    "    albumentations.Normalize(mean=(0.485, 0.12, 0.005), std=(0.081, 0.32, 0.043), max_pixel_value=255.0, p=1.0),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = albumentations.Compose([\n",
    "    albumentations.Resize(IMAGENET_SIZE, IMAGENET_SIZE),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomGamma(gamma_limit=(60, 120), p=0.9),\n",
    "        albumentations.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.9),\n",
    "        albumentations.RandomBrightness(limit=0.2, p=0.9),\n",
    "        albumentations.RandomContrast(limit=0.2, p=0.9)\n",
    "        ]),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, interpolation=cv2.INTER_LINEAR,border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(p=0.2),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "        albumentations.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=1)\n",
    "    ], p=0.5),\n",
    "    albumentations.Normalize(mean=(0.485, 0.12, 0.005), std=(0.081, 0.32, 0.043), max_pixel_value=255.0, p=1.0),\n",
    "])\n",
    "\n",
    "val_transform = albumentations.Compose([\n",
    "    albumentations.Resize(IMAGENET_SIZE, IMAGENET_SIZE),\n",
    "    # albumentations.CenterCrop(512, 512),\n",
    "    # albumentations.JpegCompression(quality_lower=99, quality_upper=100,p=1),\n",
    "    albumentations.Normalize(mean=(0.485, 0.12, 0.005), std=(0.081, 0.32, 0.043), max_pixel_value=255.0, p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_labels, transform=None, target_transform=None):\n",
    "        self.img_labels = img_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_labels.iloc[idx, 0]\n",
    "        image = Image.open(img_path)\n",
    "        # image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        # sample = {'image':image, 'label':label}\n",
    "        if self.transform:\n",
    "            image_np = np.array(image)\n",
    "            image = self.transform(image=image_np)['image']\n",
    "            # image = Image.fromarray(augmented['image'])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_size = 224\n",
    "train_batch_size = 64\n",
    "val_batch_size = 24\n",
    "workers = 8\n",
    "\n",
    "train_dataset = CustomImageDataset(train_df, transform)\n",
    "val_dataset = CustomImageDataset(val_df, val_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,        \n",
    "    shuffle=True,\n",
    "    # sampler=ImbalancedDatasetSampler(train_dataset),\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=val_batch_size,        \n",
    "    shuffle=False,\n",
    "    num_workers=workers,\n",
    "    pin_memory=False,\n",
    "    drop_last=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = {'train':train_loader,'val':val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_auc = 0.0\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.permute(0, 3, 1, 2)\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} {dataset_sizes[phase]} {running_corrects} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            # deep copy the model\n",
    "            global best_acc\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.densenet121(pretrained=True)\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model = nn.DataParallel(model_ft)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 20628, 'val': 5158}\n",
      "Epoch 0/19\n",
      "----------\n",
      "train 20628 19164 Loss: 0.1848 Acc: 0.9290\n",
      "val 5158 4844 Loss: 0.1559 Acc: 0.9391\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train 20628 19200 Loss: 0.1796 Acc: 0.9308\n",
      "val 5158 4840 Loss: 0.1567 Acc: 0.9383\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train 20628 19172 Loss: 0.1820 Acc: 0.9294\n",
      "val 5158 4849 Loss: 0.1562 Acc: 0.9401\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train 20628 19215 Loss: 0.1801 Acc: 0.9315\n",
      "val 5158 4843 Loss: 0.1569 Acc: 0.9389\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train 20628 19175 Loss: 0.1825 Acc: 0.9296\n",
      "val 5158 4846 Loss: 0.1562 Acc: 0.9395\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train 20628 19173 Loss: 0.1807 Acc: 0.9295\n",
      "val 5158 4843 Loss: 0.1563 Acc: 0.9389\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train 20628 19183 Loss: 0.1785 Acc: 0.9299\n",
      "val 5158 4838 Loss: 0.1570 Acc: 0.9380\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {'train':len(x_train),'val':len(X_test)}\n",
    "print(dataset_sizes)\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft,'best_model_1119.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_table = pd.read_csv('/data/VPS/VPS_03/lmq/pancreas/data/pancr_net_test1.csv')\n",
    "test2_table = pd.read_csv('/data/VPS/VPS_03/lmq/pancreas/data/pancr_net_test2.csv')\n",
    "test1_dataset = CustomImageDataset(test1_table, val_transform)\n",
    "test2_dataset =  CustomImageDataset(test2_table, val_transform)\n",
    "\n",
    "test1_loader = torch.utils.data.DataLoader(\n",
    "    test1_dataset,\n",
    "    batch_size=1,        \n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.array(0,2)\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
